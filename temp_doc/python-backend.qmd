---
title: "Rapid prototyping for a Python backend"
authors: "EuBIC r-python-hackathon team, Johannes Rainer"
date: last-modified
format:
    html:
        code-fold: true
        code-overflow: wrap
        code-tools: true
        graphics: yes
        toc: true
        toc-location: left-body
        toc-title: "Outline"

---


# Introduction

Instead of translating and hence copying the MS data between R and Python we
could also have a Python-based `MsBackend` for `Spectra` where all data is in
Python (e.g. as a list of `matchms.Spectrum` objects) and we retrieve the data,
as basic data types, whenever needed. That could be more efficient and also less
memory demanding. Data analysis in Python can be done directly on the
variable/object in Python, while we can access and use the data in R through the backend.

Related issue: https://github.com/rformassspectrometry/SpectriPy/issues/33

Load libraries

```{r}
library(reticulate)
library(SpectriPy)
library(Spectra)

```

Note: no `py` variable available at this stage! **Update** when `py` can be
initialized when putting `py_available(TRUE)` into the package's `.onLoad()` function.

```{r}
py

SpectriPy:::.get_py()
py_available()
```

We can initialize the Python module using `py_available()`.

```{r}
py_available(TRUE)
```


## Loading data in Python

We first load the data in Python go the other way round: reading and importing
the data from Python. The data file is within the package, thus we first define
the file name using R.

```{r}
fl <- system.file("extdata", "spectra2.mgf", package = "SpectriPy")
```

Next we load the data using *matchms* in Python.

```{python}
import matchms
from matchms.importing import load_from_mgf

pysps = list(load_from_mgf(r.fl))

```

The data is now loaded in Python.

```{python}
len(pysps)
pysps[1].peaks.to_numpy
```

And we can also get access to the data from R.

```{r}
length(py$pysps)
py$pysps[[1]]$peaks$to_numpy |>
    head()
```

What's the more efficient way to access the data?

```{r}
library(microbenchmark)

microbenchmark(
    py$pysps[[1]]$peaks$to_numpy,
    py_run_string("vals = pysps[0].peaks.to_numpy")$vals,
    r_to_py(py_run_string("vals = pysps[0].peaks.to_numpy", convert = FALSE))
)

```

The `py_run_string()`, although looking less clean, is faster. And just to
ensure, the results are identical.

```{r}
library(testthat)
expect_identical(
    py$pysps[[1]]$peaks$to_numpy,
    py_run_string("vals = pysps[0].peaks.to_numpy")$vals)
```

Getting all peaks matrices from the python object.

```{python}
res = list()
for i in range(len(pysps)):
  res.append(pysps[i].peaks.to_numpy)
```

Access that variable from R.

```{r}
class(py$res)
class(py$res[[1L]])

```

Check if the attribute is available.

```{r}
py_has_attr(py, "res")
```

Next we create our `MsBackendPython` backend pointing to that data.

```{r}
be <- backendInitialize(MsBackendPython(), pythonVariable = "pysps")
be
```

We put that into a `Spectra`.

```{r}
s <- Spectra(be)
```

And we can easily access the data:

```{r}
peaksData(s)[[2]] |>
    head()
```

The data is still in Python, and the *translation* of the data is done
on-the-fly using `r_to_py()` on the core data types (numpy arrays) when
requesting the data. The only *issue* is we don't have any column names on our
peaks matrices.




## Subsetting

Subsetting will be done directly on the data in Python. Since it's possible to
have two copies of a `Spectra` (or the backend) in R pointing to the same data
in Python, we need to eventually check that lengths match.
